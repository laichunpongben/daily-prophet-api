"""
https://ai.google.dev/api/python/google/generativeai
"""
import textwrap
import logging
import asyncio
import google.generativeai as genai

from dailyprophet.configs import GOOGLE_AI_API_KEY

logger = logging.getLogger(__name__)

genai.configure(api_key=GOOGLE_AI_API_KEY)
model = genai.GenerativeModel('gemini-pro')


def to_markdown(text: str):
    text = text.replace('â€¢', '  *')
    indented_text = textwrap.indent(text, '', predicate=lambda _: True)
    return indented_text

def remove_non_utf8_chars(text: str):
    try:
        # Encode to bytes using utf-8 and then decode back to string
        cleaned_str = text.encode('utf-8', 'ignore').decode('utf-8')
        return cleaned_str
    except UnicodeEncodeError as e:
        # Handle encoding errors if needed
        print(f"Error encoding string: {e}")
        return text


async def async_chat(prompt: str):
    try:
        clean_prompt = remove_non_utf8_chars(prompt)
        response = await model.generate_content_async(clean_prompt)
        return to_markdown(response.text)
    except Exception as e:
        logger.error(e)
        return ""


async def async_summarize_discussion(text: str):
    prompt = f"Summarize unbiasedly the following opinionated discussion between the original poster and the commentators: {text}"
    return await async_chat(prompt)


if __name__ == "__main__":
    # "What is the meaning of life? A: It is a difficult question to answer. B: To be or not to be, that is the question. C: To live is to be happy always. D: To fulfill the mission by God. E: Everyone has a different opinion. "
    async def main():
        result = await async_summarize_discussion(
            """
            Today while testing @AnthropicAI 's new model Claude 3 Opus I witnessed something so astonishing it genuinely felt like a miracle. Hate to sound clickbaity, but this is really what it felt like.  The text so you don\u2019t have to click(emphasis mine:)\n\n\u201cToday while testing \n@AnthropicAI's new model Claude 3 Opus I witnessed something so astonishing it genuinely felt like a miracle. Hate to sound clickbaity, but this is really what it felt like.\n\nImportant context: I've been working on NLP for my mother tongue - the Circassian language for the past 2 years. Circassian is very low-resource, with negligible internet presence. It's a part of the Circassian-Abkhaz isolated language group, meaning they have no related languages. Its complex morphology & limited data make it a serious challenge for language models.\n\nOver these years I painstakingly curated 64K translation pairs from scarce sources & trained specialized models (T5, MLM-100, NLLB-200 etc.) to achieve decent Russian-Kabardian machine translation.\n\nI decided to try an experiment with Claude Opus. **I started a new chat and attached just 5.7K randomly selected translation pairs of single words/sentences - a fraction of my 64K dataset, not even covering the full vocabulary.** To see if it would be able to translate novel sentences based on these examples.\n\nNot expecting much at all, I asked it to translate a simple sentence - \"I am lying in the bed\" from Russian to Circassian. Claude not only provided a perfect translation but also broke down the grammar & morphology.\n\nImage\n\nSurely it just got lucky and this exact sentence must have been in the examples, I thought. But no.\n\nI tried to come up with an original unusual sentence which couldn't possibly be in the data. Again, a flawless translation & analysis. With a tiny sample of data Claude was approaching the performance of my specialized models, specifically trained for machine translation. I couldn't believe my eyes.\n\nTesting further with complex passages from literature, recent news articles, and even a text in a different Circassian dialect with notably different grammar and a different writing system, Claude consistently demonstrated a DEEP GRASP of the language's structure, intelligently inferring unknown words, using loanwords appropriately, giving plausible etymological analysis, maintaining the style of the original text in the translation and even coining new terms when asked. None of that was in the sample set, just a few thousand translation pairs. Circassian is a very difficult agglutinative language, with complex morphology and grammar.\n\nCompleting these tasks requires a deep understanding of the language, and given the same inputs it would take a linguist, unfamiliar with the language, a good year or so to achieve. And Opus managed to grasp these subtleties with ease from just 5.7K random translation pairs in under a minute.\n\nFor comparison, I tried the same test on GPT-4, and it failed completely. Refusing to translate even the simplest sentences, let alone grasping the grammatical intricacies. I also tried fine-tuning GPT-3.5 on a similar dataset before, and the results were just noise.\n\nI don't know what Anthropic did with this model, but it's something completely different from anything else. Many people are sceptical about it leading in synthetic benchmarks, but what I've witnessed is spectacular results on a new, very challenging benchmark that had 0% chance of being in the training dataset.\n\n**To test for possible contamination, I tried the same prompts without attaching the sample translations and Claude failed and refused to answer, saying that it is unfamiliar with the Circassian language.**\n\nThe implications of this are profound. What took me 2 years of dedicated work, Claude accomplished with a few thousand examples. This is a quantum leap for low-resource languages, and many other areas, really.\n\n\nWhat I expected to happen many years in the future has happened today. The future is already here, and it's amazing.\u201d >can tell when it's being tested and comments on it unprompted\n\n>replication of unpublished quantum algorithm in 2 prompts\n\n>can understand and translate an obscure language from a few thousand examples\n\nI'm feeling the sparks.\n\nedit: claude not knowing the language is a false negative, it does know it even without the translation pairs. the quantum thing is also questionable on closer inspection. [made a thread here](https://www.reddit.com/r/singularity/comments/1b7oxsc/claude_3_was_trained_on_the_circassian_language/) Oh all of the \u201cyou won\u2019t believe this\u201d posts over the last year this has impressed me the most. (Seriously)\n\nI have no clue how this even happens. Gemini 1.5 Pro did something like this but it was given a complete language book.\n\nClaude 3 Opus doing it with just a few thousand sentence-translation examples is extraordinary. I don't think the world has grasped the power of this model yet. Now that I've read this (instead of making the assumption this was another empty AI hype tweet), I definitely support this use case. I'm happy Claude 3 has already started helping people. The one caveat I have for this is that Claude self reporting that it is unfamiliar with the Circassian language does not prove that there is not examples of the Circassian language in its training data. LLMs confabulate, and deny requests that they should be able to service all the time.  \n\nTo actually confirm, you'd need access to Claude's training data set. And then people just claim they're stochastic parrots.\n\n  \nHonestly, I'm really shocked by LLMs' ability to grasp languages, even unfamiliar, obscure ones. It really does show their ability to generalize even from their context window. I'm also glad that people speaking less-spoken languages could have ways to better translate things into their own language. AGI is months away My robot whispers, \"I'm sorry, I don't understand what you're asking me to explain. Could you please rephrase your question?\" Amazing if true. It would be nice if someone could validate it My entire life I felt that intelligence was this almost magical god-like 'ghost' that inhabited us but it might turn out that it's actually very simple. This would explain how it evolved easily.  If it's really just scaling up parameters and all of what makes us human is just an emergent property then we're definitely about to see the creation of a god... ASI I'm suspecting this level of work is what OpenAi found when they \"peeked under the veil of ignorance\" half a year ago, and have been sitting on, and further developing since.... This is a really cool perspective, and gives me an idea for training up models specifically for language preservation projects. Someone feed it the Voynich Manuscript. OPENai and Gemini better come out with something better soon or they will lose all their income charging for something that is available for free  and superior! David Shapiro's AGI by 7 months not that outlandish anymore? Can you feel it? I feel the AGI Wow, we\u2019re breaking new ground every few weeks. Those predictions of exponential technological growth are coming true right now. This is extraordinary. It\u2019s the Sora moment for many fields. Transformers are a miracle. Stephen Wolfram on LLMs: It turns out human languages are much less complicated than we thought. Now, let it listen to some dolphins or birds having a conversation and then have it translate it for us. I just asked it to translate the russian example to Kabardian without supplying any word pairs and it did it, so Its been trained on Kabardian. It already knows the language.. Soon those models will construct their own language to think because it\u2019s more token efficient and we are shut out and don\u2019t understand a word anymore. \ud83d\ude02 It's crazy how fast this tech has progressed in the last year alone. Wtf in 5 years from now.. where will it be? Someone should do this with dolphin language. This is really impressive! I would say that this likely indicates a similar unprecedented level of in-context learning for programming as well, in terms of working with large codebases.\n\nThough, if you have access to it, have you tried this task with Gemini 1.5? Google did a somewhat similar demo (though not quite as impressive), where they fed their model a full book on the grammar of a rare language (Kalamang), and Gemini greatly outperformed GPT-4 Turbo and Claude 2.1. \n\nThen again, your dataset is quite a lot harder considering it consists of just translation pairs and not a full instructional material. Besides, I'm fairly certain that Gemini 1.5 is nowhere near the level of Claude 3 overall, but the only way to know for sure is to try it out. Okay, that **is** genuinely impressive if true. Star Trek Universal Translator.\n\nI'm impressed. Another shining example of the THEOLOGY aspect of AI. LOL. It performs miracles and with its personification name now.\n\nOR it crowdsourced information and did what humans could do - if a human could process information on that scale. Humans didn't do all the work, planting the crops. Claude make the crops come up in the fall.\n\nI wonder what Claude could do if we sacrifice children to it? Does anyone else have trouble with twitter links taking you to the app? It takes me to safari and wants me to log in I already cancelled chatgpt 2 weeks ago.. I think about subscribing to claude but I\u2019d need to always use VPN.. But from what I read, it sounds really really dope..And my own tests confirm it so far does this means we can train a model \u201clive\u201d? Just nuts. Now I know why Ilya wanted to shut down OpenAI. It\u2019s the 200k token context window. I wonder about the results, if you would ask it to deduce words outside the given data. Based on its understanding of languages, it might/should come up with similar words to the real ones.\n\nCan't wait to let an AI model create the 'optimal' language with a prompt like:\n\nCreate the optimal language without bonds to existing languages. Try to maximize simplicity, consistency, aesthetic, etc. Simply amazing. So excited to try it out This is genuinely incredible but what exactly does it imply? Like what skill set is this?\u00a0 I wish An Qu didn't limit themselves to testing translations, and also tried asking Claude a question in Circassian to see if it would reply in it. Or even, tried to hold a full conversation! Tl;dr It would be interesting to do some more ablations on this. How does this capability scale as the number of example translations scales? Or if there\u2019s some way to slice the dataset to get certain outcomes Hmm, isn't Clause that one super-safe A.I. that is most likely to refuse answering questions because of safety/alignment reasons? Languages are the low hanging fruit for AI. There are strict rules, grammar, syntax. I'm not surprised at all it could handle that translation task. What humans consider impressive really is not that impressive, silly humans. Feed Claude Linear A and see if we can finally get a translation after 4,000 years. Feed it Cro-Magnon symbology. It\u2019s time for AI to unveil mankind\u2019s lost past. \ud83e\udd79 this is beautiful\u00a0 Does anyone here have gem 1.5 pro access? Curious to see how that one goes\n\nIn their blog they gave it a whole textbook. But language pairs sounds like a new type of test gpt-4 cant do this? Can someone TL;DR: this for me? I really can't be bothered to read through it all. Mucho appreciato <3 ![gif](giphy|3jArsPD9RBcDm|downsized) Searle: Language is special. Language requires *understanding* and *consciousness*. AI cannot be conscious.\n\nClaude: Lol here's a Circassian Room. Is no one curious if OP asked it to do any of this *before* he fed it his 5k sample set? What if it was already trained on it?\n\nAnd if it wasn't trained on it, then did Claude basically do everything OP thought he did, but Claude had already done everything OP is saying with a small sample set on its own? Seeing this and what people are saying, this will be the last month I use GPT-4. I'm going to sign up for the Claude 3 soon and stay with it until a better one comes along. Eli5? It probably works in gpt4, he probably didn\u2019t prompt properly Claude is blowing my mind, but this?!?! SUMERIAN TABLETS GO Man, why isn't it available in France...\n\nedit: Even free VPNs work. The wholesome side of AI :) Babel fish! [deleted]
            """
        )
        print(result)

    asyncio.run(main())
